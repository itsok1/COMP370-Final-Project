# -*- coding: utf-8 -*-
import time
import csv
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup, NavigableString
import re
from bs4 import NavigableString

# æ­£åˆ™åŒ¹é… NPR æ—¥æœŸæ ¼å¼ï¼Œå¦‚ "February 20, 2025"
DATE_REGEX = re.compile(r"[A-Za-z]+ \d{1,2}, \d{4}")

BASE_URL = "https://www.npr.org/search?query=zelenskyy&sortType=bestMatch&page={}"




# å¤šç§æ—¥æœŸæ ¼å¼
DATE_REGEXES = [
    re.compile(r"[A-Za-z]+ \d{1,2}, \d{4}"),                    # November 17, 2025
    re.compile(r"\d{4}-\d{2}-\d{2}"),                           # 2025-11-17
    re.compile(r"[A-Za-z]+ \d{1,2}, \d{4}\s*\d{1,2}:\d{2}"),    # November 17, 2025 4:53
]


def match_date(text):
    """å°è¯•æ‰€æœ‰æ­£åˆ™"""
    if not text:
        return None
    for reg in DATE_REGEXES:
        m = reg.search(text)
        if m:
            return m.group(0)
    return None


def extract_date_near(a):
    """ç»ˆæç‰ˆï¼šä» NPR æœç´¢ç»“æœä¸­æ‰¾æ—¥æœŸ"""

    # ------------ 1. åœ¨ <a> çš„çˆ¶çº§å—å†…æ‰¾æ—¶é—´ ------------
    parent = a.find_parent(["article", "div"])
    if parent:
        for node in parent.find_all(["time", "span", "p", "div"]):
            text = node.get_text(" ", strip=True)
            found = match_date(text)
            if found:
                return found

    # ------------ 2. æ£€æŸ¥å…„å¼ŸèŠ‚ç‚¹ ------------
    for sib in a.next_siblings:
        text = sib.get_text(" ", strip=True) if not isinstance(sib, NavigableString) else str(sib).strip()
        found = match_date(text)
        if found:
            return found

    # ------------ 3. åœ¨ input, meta çš„ value ä¸­æ‰¾ ------------
    for tag in a.find_parent(["div", "article"]).find_all(["input", "meta"]):
        val = tag.get("value") or tag.get("content")
        if val:
            found = match_date(val)
            if found:
                return found

    # ------------ 4. åœ¨æ›´ä¸Šå±‚çš„å—æŸ¥æ‰¾ ------------
    block = a.find_parent("article") or a.find_parent("div")
    if block:
        text = block.get_text(" ", strip=True)
        found = match_date(text)
        if found:
            return found

    # ------------ 5. æœ€åå…œåº•ï¼šå…¨æ–‡æœç´¢é™„è¿‘ ------------
    all_text = a.find_parent("body").get_text(" ", strip=True)[:3000]
    found = match_date(all_text)
    if found:
        return found

    return None



def extract_articles_from_html(html):
    """ä»æµè§ˆå™¨æ¸²æŸ“åçš„ HTML ä¸­æå–æ–‡ç« """
    soup = BeautifulSoup(html, "html.parser")
    results = []

    # NPR æ–‡ç« é“¾æ¥éƒ½ä»¥ https://www.npr.org/20xx å¼€å¤´
    for a in soup.select('a[href^="https://www.npr.org/20"]'):
        url = a["href"]

        # æ’é™¤æ ç›®é¡µï¼ˆä¸€èˆ¬ä»¥ / ç»“å°¾ï¼‰
        if url.endswith("/"):
            continue

        title = a.get_text(strip=True)
        if not title:
            continue

        # æå–æ—¥æœŸ
        date = extract_date_near(a)

        results.append({
            "url": url,
            "title": title,
            "date": date
        })

    return results


def extract_opening(browser, url):
    """ç¨³å®šæå– NPR æ­£æ–‡å‰ 1ï½2 æ®µ"""

    page = browser.new_page()
    try:
        page.goto(url, timeout=20000)
        page.wait_for_timeout(2000)
    except:
        return ""

    soup = BeautifulSoup(page.content(), "html.parser")

    # 1ï¸âƒ£ ä¼˜å…ˆæŠ“ storytext
    paragraphs = soup.select("div[data-testid='storytext'] p")

    # 2ï¸âƒ£ fallbackï¼šæŠ“ article ä¸‹çš„ <p>ï¼ˆä½†é¿å…æŠ“ authors / summaryï¼‰
    if not paragraphs:
        paragraphs = soup.select("article p")

    clean_paras = []
    for p in paragraphs:
        text = p.get_text(strip=True)

        # è¿‡æ»¤æ‰éæ­£æ–‡æ®µè½
        if not text:
            continue
        if text.startswith("By "):
            continue
        if text.startswith("Subscribe"):
            continue
        if "Up First" in text and len(text) < 200:
            continue
        if "NPR" in text and len(text) < 200:
            continue

        clean_paras.append(text)

        if len(clean_paras) >= 2:
            break

    page.close()

    return " ".join(clean_paras)



def main():
    with sync_playwright() as p:
        browser = p.firefox.launch(headless=True)
        page = browser.new_page()

        all_articles = []
        seen = set()

        MAX_ARTICLES = 200
        page_num = 1

        while len(all_articles) < MAX_ARTICLES:

            url = BASE_URL.format(page_num)
            print(f"æ‰“å¼€ç¬¬ {page_num} é¡µ: {url}")

            page.goto(url)
            page.wait_for_timeout(2500)

            # ç‚¹å‡» Cookie å¼¹çª—ï¼ˆJS å¼ºåˆ¶ï¼‰
            try:
                page.evaluate("""
                    const btn = [...document.querySelectorAll('button')]
                        .find(b => b.textContent.trim() === 'Allow All');
                    if (btn) btn.click();
                """)
            except:
                pass

            html = page.content()
            items = extract_articles_from_html(html)

            if not items:
                print("æ­¤é¡µæ— æ–‡ç«  â†’ åœæ­¢")
                break

            # æ·»åŠ å»é‡åçš„æ–‡ç« 
            for it in items:
                if it["url"] not in seen:
                    seen.add(it["url"])
                    all_articles.append(it)

                if len(all_articles) >= MAX_ARTICLES:
                    break

            print(f"å½“å‰æ€»æ•°: {len(all_articles)} ç¯‡\n")

            page_num += 1
            time.sleep(1)

        print("å¼€å§‹æŠ“æ­£æ–‡å¼€å¤´â€¦â€¦")

        for i, art in enumerate(all_articles):
            print(f"[{i+1}/{len(all_articles)}] {art['url']}")
            art["opening"] = extract_opening(browser, art["url"])
            time.sleep(1)

        # ä¿å­˜ CSV
        with open("npr_zelensky_bestmatch_200.csv", "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(["date", "title", "url", "opening"])
            for a in all_articles:
                writer.writerow([a["date"], a["title"], a["url"], a["opening"]])

        print("\nğŸ‰ å®Œæˆï¼å·²ä¿å­˜åˆ° npr_zelensky_bestmatch_200_1.csv")
        browser.close()


if __name__ == "__main__":
    main()
